{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs the zero-shot experiments in \"A Robust and Efficient Stopping Criteria for Systematic Reviews Using Poisson Processes.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.optimize import curve_fit\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# IMPORT EXPERIEMENTAL FUNCTIONS\n",
    "from utils.read_data_fns import *\n",
    "from utils.target_method_fns import *  \n",
    "from utils.inhomogeneous_pp_fns import *   \n",
    "from utils.eval_fns import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ TOPIC RELEVANCE DATA\n",
    "with open('data/relevance/qrel_abs_test.txt', 'r') as infile:\n",
    "    qrels_data = infile.readlines()    \n",
    "query_rel_dic = make_rel_dic(qrels_data) # make dictionary of list of docids relevant to each queryid\n",
    "all_runs = glob.glob('data/runs2017_table3/*/*')    \n",
    "\n",
    "\n",
    "# SET POISSON PROCESS PARAMETERS\n",
    "sample_props = [0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,\n",
    "                0.7,0.75,0.8,0.85,0.9,0.95,1]  # proportion of docs to sample\n",
    "min_rel_in_sample = 20 # min number rel docs must be initial sample to proceed with algorithm \n",
    "n_windows = 10  # number of windows to male from sample\n",
    "\n",
    "\n",
    "# SET EXPERIMENTAL PARAMETERS \n",
    "des_recalls = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95] # desired recalls to experiment over\n",
    "des_probs = [0.7, 0.95] # desired probabilities to experiment over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn to run stopping methods (baseline, oracle, target method and poisson process over all runs)\n",
    "def run_sp_algorithms(des_recall, des_prob):\n",
    "    \n",
    "    # PREPARE SCORING DICTIONARIES\n",
    "    run_score_dic = {}\n",
    "    oracle_dic = {}\n",
    "    \n",
    "    # LOOP OVER RUNS\n",
    "    for run in sorted(all_runs):\n",
    "\n",
    "\n",
    "        # MAKE DATA DICTIONARIES\n",
    "        with open(run, 'r') as infile:\n",
    "            run_data = infile.readlines()\n",
    "        doc_rank_dic = make_rank_dic(run_data)  # make dictionary of ranked docids for each queryid\n",
    "        rank_rel_dic = make_rank_rel_dic(query_rel_dic,doc_rank_dic) # make dic of list relevances of ranked docs for each queryid\n",
    "\n",
    "        # PREPARE SCORING DICTIONARIES\n",
    "        run_name = run[5:]\n",
    "        score_dic = {}\n",
    "        oracle_dic[run_name] = []\n",
    "\n",
    "        # LOOP OVER TOPICS\n",
    "        topics_list = make_topics_list(doc_rank_dic,1)  # sort topics by no docs\n",
    "        for query_id in topics_list:\n",
    "            score_dic[query_id] = []      \n",
    "\n",
    "            # EXTRACT COUNTS AND REL LISTS\n",
    "            n_docs = len(doc_rank_dic[query_id])  # total n. docs in topic\n",
    "            rel_list = rank_rel_dic[query_id]  # list binary rel of ranked docs \n",
    "\n",
    "\n",
    "            # ORACLE\n",
    "            rel_doc_idxs = np.where(np.array(rel_list) == 1)[0]\n",
    "            orcale_n_rel = math.ceil(len(rel_doc_idxs)*des_recall)\n",
    "            oracle_idx = rel_doc_idxs[orcale_n_rel-1]\n",
    "            oracle_eff = oracle_idx+1\n",
    "            oracle_dic[run_name].append(oracle_eff)\n",
    "            \n",
    "            \n",
    "            # BASELINE (stop at proportion = to desired recall) \n",
    "            n_samp_docs = int(round(n_docs*des_recall))\n",
    "            bl_recall = calc_recall(rel_list, n_samp_docs)\n",
    "            bl_effort = n_samp_docs\n",
    "            bl_accept = calc_accept(bl_recall, des_recall)\n",
    "            score_dic[query_id].append((bl_recall, bl_effort, bl_accept))\n",
    "\n",
    "\n",
    "\n",
    "            # TARGET METHOD\n",
    "            random.seed(1)\n",
    "            target_size = get_target_size(des_recall, des_prob)\n",
    "            target_list, examined_list = make_target_set(rel_list, n_docs, target_size)  # get target sample and list all docs examined\n",
    "            tar_stop_n = get_stopping_target(target_list, n_docs, target_size)  # stopping point\n",
    "            all_examined_idxs = get_all_target_examined_idxs(examined_list, tar_stop_n)  # list of every doc examined during method\n",
    "            tar_recall = calc_recall(rel_list, tar_stop_n)\n",
    "            tar_effort = len(all_examined_idxs) # total effort (inc. sampling)\n",
    "            tar_accept = calc_accept(tar_recall, des_recall)\n",
    "            score_dic[query_id].append((tar_recall, tar_effort, tar_accept))\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "            # INHOMOGENEOUS POISSON PROCESS\n",
    "            # check topic meets initial relevance requirement\n",
    "            n_samp_docs = int(round(n_docs*sample_props[0]))\n",
    "            sample_rel_list = rel_list[0:n_samp_docs]  # chunck of rel list examined in sample\n",
    "\n",
    "            # if meet size requirement run algorithm; else return n_docs as stopping point\n",
    "            if (np.sum(sample_rel_list) >= min_rel_in_sample):\n",
    "\n",
    "                windows_end_point = 0\n",
    "                pred_stop_n = n_docs\n",
    "                i = 0\n",
    "\n",
    "                while (i < len(sample_props)) and (pred_stop_n > n_samp_docs):\n",
    "                    sample_prop = sample_props[i]\n",
    "\n",
    "                    n_samp_docs = int(round(n_docs*sample_props[i]))\n",
    "                    sample_rel_list = rel_list[0:n_samp_docs]  # chunck of rel list examined in sample\n",
    "\n",
    "                    # get points\n",
    "                    windows = make_windows(n_windows, sample_prop, n_docs)\n",
    "                    window_size = windows[0][1]\n",
    "\n",
    "                    x,y = get_points(windows, window_size, sample_rel_list)  # calculate points that will be used to fit curve\n",
    "\n",
    "                    good_curve_fit = 0\n",
    "                    # try to fit curve\n",
    "                    try: \n",
    "                        p0 = [0.1, 0.001]  # initialise curve parameters\n",
    "                        opt, pcov = curve_fit(model_func, x, y, p0)  # fit curve\n",
    "                        good_curve_fit = 1\n",
    "                    except: \n",
    "                        pass\n",
    "                    \n",
    "                    if(good_curve_fit == 1):\n",
    "                        a, k = opt\n",
    "                        y2 = model_func(x, a, k) # get y-values for fitted curve\n",
    "\n",
    "                        # check distance between \"curves\" at end sample\n",
    "                        n_rel_at_end_samp = np.sum(sample_rel_list)\n",
    "                        y3 =  model_func(np.array(range(1,len(sample_rel_list)+1)), a, k)\n",
    "                        est_by_curve_end_samp = np.sum(y3)\n",
    "                        est_by_curve_end_samp = int(round(est_by_curve_end_samp))\n",
    "\n",
    "\n",
    "                        if n_rel_at_end_samp >= des_recall*est_by_curve_end_samp:\n",
    "\n",
    "\n",
    "                            # using inhom Poisson process with fitted curve as rate fn, predict total number rel docs in topic \n",
    "                            mu = (a/-k)*(math.exp(-k*n_docs)-1)  # integral model_func\n",
    "                            pred_n_rel = predict_n_rel(des_prob, n_docs, mu) # predict max number rel docs (using poisson cdf)\n",
    "                            des_n_rel = des_recall*pred_n_rel\n",
    "                            if des_n_rel <= n_rel_at_end_samp:\n",
    "                                pred_stop_n = n_rel_at_end_samp             \n",
    "\n",
    "\n",
    "                    i += 1  # increase sample proportion size\n",
    "\n",
    "\n",
    "                # score result \n",
    "                inhom_recall = calc_recall(rel_list, n_samp_docs)\n",
    "                inhom_effort = n_samp_docs\n",
    "                inhom_accept = calc_accept(inhom_recall, des_recall)\n",
    "                score_dic[query_id].append((inhom_recall, inhom_effort, inhom_accept))\n",
    "\n",
    "\n",
    "            else: # if not enough rel docs in min sample, stopping point is n_docs\n",
    "                inhom_recall = calc_recall(rel_list, n_docs)\n",
    "                inhom_effort = n_docs\n",
    "                inhom_accept = calc_accept(inhom_recall, des_recall)\n",
    "                score_dic[query_id].append((inhom_recall, inhom_effort, inhom_accept))\n",
    "\n",
    "        # SCORE RESULTS\n",
    "        tar_accept_vec = [val[1][2] for val in score_dic.values()]\n",
    "        inhom_accept_vec = [val[2][2] for val in score_dic.values()]\n",
    "        BL_accept_vec = [val[0][2] for val in score_dic.values()]\n",
    "        tar_eff_vec = [val[1][1] for val in score_dic.values()]\n",
    "        inhom_eff_vec = [val[2][1] for val in score_dic.values()]\n",
    "        BL_eff_vec = [val[0][1] for val in score_dic.values()]\n",
    "        topic_size_vec = [len(doc_rank_dic[query_id]) for query_id in topics_list]\n",
    "\n",
    "        \n",
    "        # ADD SCORES TO DICTIONARIES\n",
    "        run_score_dic[run_name] = {}\n",
    "        run_score_dic[run_name]['tar rel'] = calc_reliability(tar_accept_vec)\n",
    "        run_score_dic[run_name]['in rel'] = calc_reliability(inhom_accept_vec)\n",
    "        run_score_dic[run_name]['BL rel'] = calc_reliability(BL_accept_vec)\n",
    "        run_score_dic[run_name]['tar tot eff'] =  np.sum(tar_eff_vec)\n",
    "        run_score_dic[run_name]['in tot eff'] = np.sum(inhom_eff_vec)\n",
    "        run_score_dic[run_name]['BL tot eff'] = np.sum(BL_eff_vec)\n",
    "        run_score_dic[run_name]['oracle eff'] = np.sum(oracle_dic[run_name])\n",
    " \n",
    "\n",
    "    # MAKE DATAFRAME OF ALL RESULTS\n",
    "    df = pd.DataFrame.from_dict(run_score_dic, orient='index')\n",
    "    \n",
    "    return  (df, df.mean().round(2).to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN EXPERIMENTS\n",
    "results_dict_verbose = {}\n",
    "results_dict = {}\n",
    "for r in des_recalls:\n",
    "    for p in des_probs:\n",
    "        results_dict_verbose[(p,r)], results_dict[(p,r)] = run_sp_algorithms(r, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE LATEX TABLE OUTPUT \n",
    "df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "\n",
    "def pes(eff):  # fn to calculate % of effort saved\n",
    "    saving = 117562-eff\n",
    "    return round(100*saving/117562,1)\n",
    "\n",
    "df['TM es'] = [pes(eff) for eff in df['tar tot eff'].tolist()]\n",
    "df['PP es'] = [pes(eff) for eff in df['in tot eff'].tolist()]\n",
    "df['BL es'] = [pes(eff) for eff in df['BL tot eff'].tolist()]\n",
    "df['OR es'] = [pes(eff) for eff in df['oracle eff'].tolist()]\n",
    "df['tar tot eff'] = df['tar tot eff'].astype(int)\n",
    "df['in tot eff'] = df['in tot eff'].astype(int)\n",
    "df['BL tot eff'] = df['BL tot eff'].astype(int)\n",
    "df['oracle eff'] = df['oracle eff'].astype(int)\n",
    "\n",
    "df = df.sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn to print oracle, pp and tm effort graphs for all recall levels at probability p\n",
    "def print_effort_graphs(p):\n",
    "    \n",
    "    print(\"Probability level:\", p)\n",
    "    \n",
    "    # RANK RUNS\n",
    "    df_aur = pd.read_csv(\"data/runs2017_AURC.csv\") \n",
    "    aurc = df_aur[\"AURC\"]\n",
    "    sorted_idxs = [i for i in np.argsort(aurc)[::-1]]\n",
    "\n",
    "\n",
    "    # MAKE ORACLE AND PP EFFORT GRAPH\n",
    "\n",
    "    for r in des_recalls[0:]:\n",
    "        df = results_dict_verbose[(p,r)]\n",
    "        df.loc[\"Mean\"] = df.mean()\n",
    "        df = df.round(2)\n",
    "        df = df.reset_index()\n",
    "        df = df.rename(columns={'index': 'Run'})\n",
    "        runs = df[\"Run\"][:-1] #  # [:-1] = drop mean\n",
    "\n",
    "\n",
    "        # MAKE ORACLE AND PP EFFORT GRAPH\n",
    "        oracle = df[\"oracle eff\"].tolist()[:-1]\n",
    "        in_eff = df[\"in tot eff\"].tolist()[:-1]\n",
    "        tm_eff = df[\"tar tot eff\"].tolist()[:-1]\n",
    "        sorted_oracle = [oracle[i] for i in sorted_idxs]\n",
    "        sorted_in_eff = [in_eff[i] for i in sorted_idxs]\n",
    "        sorted_tm_eff = [tm_eff[i] for i in sorted_idxs]\n",
    "        sorted_aur = [aurc[i] for i in sorted_idxs]\n",
    "\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(sorted_aur, sorted_in_eff, linestyle='-',marker='.',\n",
    "                                label = \"PP effort\")\n",
    "        plt.plot(sorted_aur, sorted_tm_eff, linestyle='-',marker='.',\n",
    "                                label = \"TM effort\")\n",
    "        plt.plot(sorted_aur, sorted_oracle, linestyle='-',marker='x',\n",
    "                                label = \"Oracle effort\")\n",
    "\n",
    "        plt.ylabel(\"Effort\")\n",
    "        plt.xlabel(\"AURC\")\n",
    "        plt.title(\"Recall level \"+str(r))\n",
    "        plt.legend()\n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_effort_graphs(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_effort_graphs(0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
